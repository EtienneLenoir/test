Pour p et ARCH c'est comb lineaire des pertubations passés (des erreurs résiduelles)
Pour Garch p est la moyenne mobile

# Loi Normale

La  densité  normale  a  longtemps  été  utilisée dans la littérature du fait  de  sa  simplicité. Pour autant,il est  aujourd’hui reconnu  que les propriétés  de la loi normale,  symétrique et  mésokurtique, ne  sont  pas  compatibles  avec les faits  stylisés (distribution conditionnelle leptokurtique  et  asymétrique) observés  généralement  sur  les  séries  de  rendements  des  actifs financiers  (Cont,  2001).

 Notons  toutefois, que supposer à tort la  normalité  peut tout de même conduire à des  estimations convergentes des paramètres d’un  modèle GARCH (principe du pseudo-maximum de vraisemblance), bien que non efficaces. Mais,dans le cas d’une application à la VaR,  la spécification de la loi conditionnelle ne  concerne  pas  uniquement le problème de l’estimation des paramètres du modèle GARCH et affecte plus directement la détermination du fractile de la distribution conditionnelle.Le choix à tort d’une spécification normale peut dès lors avoir des conséquences notables sur les estimations et les prévisions de VaR.
![5257b88d8852d039f67d86e579ef19fd.png](:/b6078e471cbd44db822916986cecf2fc)


 **Loi de Student** 

La distribution de Student  permet  de  modéliser des queues  de distribution  plus  épaisses  que  celles  de la  loi normale  (distribution leptokurtique). Plus  précisément, la  Kurtosis  de  la  distribution de Student est déterminée par le degré liberté v. Dès lors, dans le cadre des modèles GARCH, ce  paramètre  estimé  permet de capturer l’excès  de  Kurtosis  qui  ne peut pas être  expliqué par le modèle GARCH lui même. La distribution de Student standardisée est symétrique et la Skewness  est  nulle à condition  que  $v>3$.  La  distribution  est leptokurtique  dès  lors  que $v>4$.
![8f58282fa9aa59eb33c0af7e74183333.png](:/0306ddd63b45417aa24045a1a4130c94)

# Log Normlale

On sous entend souvent que la rentabilité arithmétique suivent une loie normale (prtu Markowitz)
Or une variable normalement distribués a toujours une une probabiltié positive de prendre des valeurs négatives, plus satisfaisant de prendre une rentabilité log normale (compatible avec efficience faible implique non correlation sérielle, et pas independance, qui est une hypothese plus forte mais seulement équivalent dans cas variables normales)




![0ba2f99a8ce429ed0977d7f0109eed86.png](:/889e4ae4a6f64e15a67f08113cf097c8)


**N'est pas corroborée sur le plan empirique :**
## Skeness

L'asymétrie vous permet de tester de combien la forme globale d'une distribution s'écarte de la forme de la distribution normale. 
![301f69d1476e67ca447ca3196a22d51a.png](:/a1b5318826d54b16ab8c47e3a585fea3)

L'asymétrie a les propriétés suivantes :

* L'asymétrie est une le moment mesure basée sur (en particulier, c'est le troisième moment), car elle utilise la valeur attendue de la troisième puissance d'une variable aléatoire .
* L'asymétrie est un moment central, car la valeur de la variable aléatoire est centralisée en la soustrayant de la moyenne
* L'asymétrie est un moment normalisé , car sa valeur est normalisée en la divisant par (une puissance de) l' écart type
* Comme il s'agit du troisième moment, une distribution de probabilité parfaitement symétrique autour de la moyenne aura une asymétrie nulle . En effet, pour chaque y_i supérieur à la moyenne µ , il y aura un y_i correspondant inférieur à la moyenne µ de la même valeur. Puisque la distribution est symétrique autour de la moyenne, les deux valeurs y_i auront la même probabilité. Ainsi, les paires de (y_i- µ) s'annuleront, donnant une asymétrie totale de zéro.
* L'asymétrie de la distribution normale est nulle.
* Alors qu'une distribution symétrique aura une asymétrie nulle, une distribution ayant une asymétrie nulle n'est pas nécessairement symétrique .
* Certaines distributions basées sur des ratios - la plus connue étant la distribution de Cauchy - ont une asymétrie indéfinie car elles ont une moyenne indéfinie µ . 



- La densité gaussienne est symétrique par rapport à sa moyenne alors que la densité présente une dissymétrie (skewness-3ieme moment).
La probabiltié d'occurence de valeurs très faibles(par rapport à la moyenne) est supereur à celle des valeurs très fortes.
$$
Sk=E(\frac{R-E(R)}{\sigma(R)})^3
$$
Estimateur empirique
$$
\widehat{S}k=   \frac{ \frac{1}{n-1} \sum_{t=1}^n(R-\widehat{E}(R))^3}{\widehat{\sigma}(R)^3})
$$
- $S=0$ distribution symétrique
- $S>0$ asymétrie à gauche ( proba evenement extreme positive > proba evenement extreme négatif)
- $S<0$ asymétrie à droite ( proba evenement extreme positive < proba evenement extreme négatif)

## Kurtosis

L'aplatissement est une mesure de la différence de forme entre les queues d'une distribution par rapport aux queues de la distribution normale. Alors que l'asymétrie se concentre sur la forme générale, Kurtosis se concentre sur la forme de la queue. 


Kurtosis a les propriétés suivantes :

* Tout comme l'asymétrie, l'aplatissement est une mesure basée sur le moment et c'est un moment central et standardisé .
* Parce que c'est le quatrième moment, Kurtosis est toujours positif .
* L'aplatissement est sensible aux écarts par rapport à la normalité sur les queues. En raison de la 4e puissance, les valeurs plus petites des valeurs centralisées (y_i-µ) dans l'équation ci-dessus sont grandement atténuées. En d'autres termes, les valeurs de Y proches du centre de la distribution sont désaccentuées. Inversement, les plus grandes valeurs de (y_i-µ) , c'est-à-dire celles situées sur les deux queues de la distribution sont fortement accentuées par la puissance 4. Cette propriété rend Kurtosis largement ignorant des valeurs situées vers le centre de la distribution, et rend Kurtosis sensible aux valeurs situées sur les queues de la distribution.
* L'aplatissement de la distribution normale est de 3,0 . Lors de la mesure de l'écart par rapport à la normalité, l'aplatissement est parfois exprimé en tant qu'excès d'aplatissement qui correspond au solde de l'aplatissement après soustraction de 3,0 . 
	
- La deuxieme vient du fait que la distribution empirique des log normalités contient une proportion plus forte de valeurs très inferieur à la moyenne que la distrib normale (queue épaisse à gauche, Kurtosis-4ieme moment). Une dsitribution dont kurtosis est plus forte que celle d'une loi normale est dite leptokurtique et est caractérisé par des probabilités d'occurences de valeurs extremes supérieur à la normale.
$$
K=E(\frac{R-E(R)}{\sigma(R)})^4 \\
\frac{\mu^4}{\sigma^4}=\frac{E[(X-\mu)^4]}{\sigma^4}
$$
Estimateur empirique
$$
\widehat{K}=   \frac{ \frac{1}{n-1} \sum_{t=1}^n(R-\widehat{E}(R))^4}{\widehat{\sigma}(R)^4})
$$
- La distribution empirique des logsR et distribution gaussienne réside en ce que le maximum de la densité des log rentabilité est superieur au maximum de la densité gaussienne de m écart type, histogtamme plus étroit autour de ce maximum

$E_k=K-3$
- $E_k=O$ loi normale
- $E_k>O$ leptokurtique (queue épaisse)
- $E_k< O$ platikurtique


**Statistique de JB**
$$
JB= \frac{n-2}{6}(\widehat{S}k^2+\frac{1}{4}(\widehat{K}-3)^2)
$$
si hypothese de normalité est vrai, JB est dsitribué comme un chi2 à deux dégré de liberté

# Modélisation par processus stochastique

Le prix S(t) d'une action évolue dans le temps de maniere aléatoire, càd il suit un processus stochastique
En temps discret on ne considère que des valeurs à des instants particulier (S(0, S(1)).
En temps continu S(t) est observé pour toutes les valeurs de t dans un intervalle

Modèle le plus sumple est consideré que les variations de prix
$$
\Delta S = S(t+ \Delta t)- S(t)
$$
dans des intervalles de temps successifs (t, t+Δ) sont independants les unes des autres, on dit alors que les prix  suivent une marche aléatoire, implique normalité des prix (en vertu duth centrale limite).
En temps continu le processus des prix s'écrit comme un mouvement brownien arithmétique. Or comme on l'a vu on prefere log normalité des rendements en dépit des inconvéniants empiriques cités avant.
$$
\Delta lnS = lnS(t+ \Delta t)- lnS(t)
$$
En supossant finies l'esperence m et la variance des accroissements
$$
E(lnS(t)-lnS(0))=\sum_{s=1}^tm ;;Variance(lnS(t)-lnS(0))=\sigma^2t
$$
L'esperence et la variance des log prix augmentent donc linéairement avec le temps. On peut également conclure que (lnS(t+Δt)-lnS(t)) est distribué selon N(mΔt,σ^2 Δt) et par conséquent :
$$
lnS(t+ \Delta t)- lnS(t)=  m\Delta t+\sigma \Delta W
$$
avec ΔW distribué selon N(0;σ^2Δt)

# EDS du mouvement brownien arithmétique de lnS(t) ( géometrique pour S(t) )
$$
dlnS(t)=  m\Delta t+\sigma \Delta W
\\
lnS(t)= lnS(0)+ m t+\sigma \Delta W
$$
ou W(t) est un mvt brownien standard( ou processus de Wiener)
(unmvt brownien standars est caractéisé par des accroissements gaussiens, independants,d'esperence nulle et variance= à la durée de l'intervalle sur lequel ils sont calculés)
La solution de cette EDS est :
$$
S(t)=S(0)e^{mt+\sigma W(t)}
$$
En appliquat le lemme d'ito à lnS :
$$
\frac{dS}{S}=(m+ \frac{\sigma^2}{2}) dt+ \sigma dW(t)
\\
\mu=m+0.5 \sigma^2 \\

\text{Equation mouvement brownien géométrique } \\
\frac{dS}{S}=\mu dt+ \sigma dW(t) \\
S(t)=S(0)e^{(\mu-0.5+\sigma^2 )\sigma W(t)}
$$
Le mouvement s'arrete quand S=0,le cours demeure nulle indefiniment, on dit que le processus possède une barriere absorbante en 0
- la composante mudtt est non aleatoire et sigmadW est aleatoire d'esperence nulle et de variance σ2dit


Normalement mvt brownien géometrique cas partiulier d'un processus d'ito qui s'écrit :
$$
\frac{dS}{S}=\mu(t) dt+ \sigma(t) dW(t)
$$
ou esperence instantanée et la volatilité ne sont pas constante mais variable(eventuellment aléatoire)
### esperence de la rentabilité
$$
\mu(t)= r(t)(tauxsansrisk)+ \theta (t) (primerisk) \\
\theta (t)= prixdumarchédurisque \times risque affectantaction
$$
Hypothese mu cst tel que dans modèle BS standars
En réalité les taux d'interet varie constamment, un accroisement du taux d'interet sans risue entraine une dimunition instantanée du prix S(t)(car valeir présente des dividendes futurs)

Prime de risque cst (pris de marché cst etc etc moins démenti à court terme à court termee et hors crise que prime volatile)
En géneral une méthode simple d'estimation, consiste à estimer prime de risque du marché (E(Rm)-r), Rm lui meme estime comme la moyenne des excèes de rentabilité sur une longue période(eurostosk, cac40..) et r(t) taux sr sur plusieurs maturités. On obtient en géneral une prime de marché entre 5 et 8$.
Pour estime la prime de marché d'un action X, on la multiplie par le beta estime de l'action (par régression lineaire entre rx et rm)

# Estimation de la volatilité

Une estimatio sans biais du taux de variance quotidien à partir des m observations les plus récentes.
$$
U_i=ln(\frac{S_i}{S_{i-1}}) \\
\sigma_n^2=\frac{1}{m-1}\sum_{i=1}^{m}(U_{n-i}-\mu)^2

$$


Les modèles simples l'estime àl'aide des estimateurs standards
$$
\sigma_p= \sqrt{\frac{1}{N-1} \sum_{i=1}^N(R_i-\mu_p)^2 } \\
\mu_p=\frac{1}{N}\sum_{i=1}^N R_i
$$
Parfois $\mu_p=0$ car la variation esperé d'une variable sur une journée est très petite par rapport à l'écart type des variations

## Modèle variable de la volatilié (EWMA)

Le lissage exponentiel utilise une logique similaire à la moyenne mobile, mais cette fois, un poids décroissant est attribué à chaque observation. Autrement dit, moins d'importance aux observations à mesure qu'on s'éloigne du présent. 

Affecte un poids plus important aux observations les plus récentes qu'aux plus anciennes, il applique des poids exponentiellement décroissants( les pondérations diminuent exponentiellement à mesure que l'on remontre dans le temps) :
$$ 
\sigma_n^2=\lambda \sigma_{n-1}^2+(1-\lambda)U_{n-1}^2\\
\sigma_p= \sqrt{\frac{1-\lambda}{1-\lambda^N} \sum_{i=1}^N \lambda^{i-1}(R_i-\mu_p)^2 }
$$
lambda <1, plus lambda faible plus, plus les observations récentes sont pondérés

Une amélioration possible est de supposer qu'il existe un taux de variance moyen de long terme auquel il convient d'affecter un certain poids.
Il parait aussi pertinent de ponderer plus fortement les données les plus récentes, ou $\alpha_i$ est kle poids attribués à la variation observée i jours au paravant.

$$
\sigma_n^2=\gamma V_l+ \sum_{i=1}^{m} \alpha_i U_{n-1}^2 \\
\sigma_n^2=W+ \sum_{i=1}^{m} \alpha_i U_{n-1}^2 \\
\gamma +\sum_{i=1}^{m} \alpha_i=1
$$

## Modèle ARCH et  GARCH

- Modèle avec hétereoscédasticité (variance non cst) des log rentabilité
-  leptokurtique (épaisseurs des queus de distributions)
- cluster de volatilité (ou persitance de la volatilité)

Toujours dans le cadre gaussien .Mais la leptokurticité ne disparait pas completement après ARCH, car effet de memoire longue sur la volatilité.
Une modélisation alternative (Mandelbrot années 60), abandonnée cadre gaussien pour des lois alpha-stables(Pareto-Levy), ont une une distribution infini conduise à des trajectoires discontinues mieux adaptés aux phénomenes plus  turbulents que mvt brownien.

L'approche s'attend à ce que la série soit stationnaire autre que le changement de variance,ce qui signifi qu'elle n'a pas de tendance ou de composant saisonnière, un modèle arch est utilisé pour prédire la variance à des pas de temps futurs
ARCH sont des processus de moyenne nulle non corrélés en série avec des variances non constantes conditionnelles au passé, mais des variances inconditionnelles constantes.Pour de tel processus, le passé récent donne des informations sur la variance des prévisions sur une période.

**R.Engle (1982) propose une modélisation des cours dont la variance change au cours du temps:**



Supposons que les log-rentabiltiés des actions soit de la forme :
$$
R_t= \mu+ \epsilon_t \\ 
\epsilon_t = z_t\sigma \text{ ou $z_t$ bruit blanc fort } 
$$
on oberve que que la variance est une combineson lineaire des $q$ dernieres perturbations obersevés élévés au carrés
$$
ARCH(1):\sigma_t^2=\alpha_0+\alpha_1\epsilon_{t-1}^2 \\
 \sigma_t^2= \omega+ \sum_{i=1}^q \alpha_i \epsilon_i^2 
$$
et parametre utilsié avec méthodes standars	s (ou maximum de vraisemblence).

Les moments conditionnels (à l'information $I_{t-1}$ disponible en $t-1$ à savoir les valeurs passés de la série des log rentabilités) de $\epsilon_t$ sont données par :
$$
E(\epsilon_t|I_{t-1})=0 \\
Var(\epsilon_t|I_{t-1})=\sigma_t^2V(z_t|I_{t-1})=\sigma_t^2=\omega+ \sum_{i=1}^q \alpha_i \epsilon_i^2 \\
var_{t-1}(\epsilon)=E[\epsilon_t^2|\epsilon{t-1}..]=	\sigma_t^2
$$
**Un choc important aura pour effet d'augmenter la variance conditionnelle du processus des erreurs et donc ainsi la probabilité de voir les chocs suivants avec une grande amplitude, ce modèle reproduit donc bien les faits stylisés de volatilité des séries finanières**

**Mais en génerale sur-estime la vrai valeur de la volatilité**
choc extremement important prenne du temps à etre integrer dans la volatilité

## Moments du processus ARCH
Ce sont ces moments conditionnels qui sont pertinents si procesus stationaires(ses lois de distribution reste inchangé par translation du temps)  :
les racines unitaires de l'équation suivante sont en dehors du cercle unitaire.

- $E(\epsilon_{t-1})=0$ , $E(\epsilon_{t-1}^2)=\alpha_0/(1-\alpha_1)=V(\epsilon_t)$
- $Var(\epsilon_t|I_{t-1})=\sigma_t^2$
$$
1-\sum_{i=1}^p\alpha_ix^i=0 \\
\sum_{i=1}^{p}\alpha_i<1  
$$
Plus nous nous projetons dans le futur et plus les prévisions du taux de variances tendent vers $V_L$

## Exemple AR(1)-ARCH(1)

$$ y_t=\mu+py_{t-1}+\epsilon_t , \epsilon_t=z_t\sigma_t^2 $$
Avec $\sigma_t^2=\alpha_0+\alpha_1\epsilon_{t-1}^2$ et $|p|<1$
Le modèle qui décrit à la fois l’évolution de l’espérance conditionnelle et de la variance conditionnelle du processus $Y_t$ dans le temps.

Les résidus $\epsilon_t$ satisfont les propriétés des processus ARCH:différence de martingale; variance conditionnelle dépendante dutemps; auto-covariance conditionnelles nulles; distribution des résidus leptokurtique

# Modèle GARCH (1986)

$$
\begin{cases} x_t=\epsilon_t \\ \epsilon_t|F_{t-1}\sim N(0,h_t) \\
h_t^2=\omega+ \sum_{j=1}^p \alpha_j h_{t-j}^2 +\sum_{i=1}^q \beta_i \epsilon_{t-j}^2 \end{cases}

$$

Modèles ARCH géneralisés par Bollerslev (1986)
**La variance est une combineson lineaire des $q$ dernieres perturbations obersevés élévés au carrés(chocs passés) ainsi que des p dernieres variances constatées(volatilié passée)**

- p : Le nombre de variances de retard à inclure dans le modèle GARCH.
- q : Le nombre d'erreurs résiduelles de décalage à inclure dans le modèle GARCH.

$$
R_t= \mu+ \epsilon_t \\ 
\epsilon_t = z_t\sigma \text{ ou $z_t$ bruit blanc fort } \\
 \sigma_t^2= \omega+ \sum_{j=1}^q \alpha_j \epsilon_{t-j}^2 +\sum_{i=1}^p \beta_i \sigma_{t-i}^2 \\
 \sum_{j=1}^q \alpha_j+\sum_{i=1}^p \beta_i <1 \\
 \gamma+ \sum_{j=1}^q \alpha_j+\sum_{i=1}^p \beta_i =1 \\
 \omega >0 ;\; \alpha_j>0  ;\beta_i>0 
$$
$E(\epsilon_{t-1}^2)=\alpha_0/(1-(\alpha_i+\beta_i))=V(\epsilon_t)$
Garch tient compte de la tendance à revenir vers son niveau de moyen de long terme $V_L$
un peu simililaire à $dV=a(V_l-V)dt+\epsilon v dz$



## Méthodes du maximum de vraisemblance

Pour l'estimation des paramètres du modèle, consiste à rechercher la valeur des paramètres maximisant la probabilité d'occurence des données historique

ex: on a 10 titres, 1 seul augmente, les autres dimininues ou sont stables, quelle est la proba de baisse du prix d'un titre sur une journée->10%.
Avec $p$ proba proportion de titres baisse, donc proba qu'1 titre baisse et les autres augmentent sont $p(1-p)^9$, ave maximum de vraisemblance la meilleur estimation de p est celle qui maximise $p(1-p)^9$

La probabilité d'estimer $u_i$ observations de la variance d'une variable $X$ normale de moyene nulle est déterminer par la fct de densité de X, avec v la variance
$$
\frac{1}{\sqrt{2\pi v}}e^{\frac{-u_i^2}{2v}}
$$
La vraisemblence d'occurence des m observations dans leur ordre d'apparition est :
$$
\prod_{i=1}^{m}[\frac{1}{\sqrt{2\pi v}}e^{\frac{-u_i^2}{2v}}]
$$
La meileur estimation de v est la valeur qui maximise cette expression
$$
\sum_{1=1}^{m}[-ln(v)-\frac{-u_i^2}{v}] \\
-mln(v)-\sum_{1=1}^{m}\frac{-u_i^2}{v}
$$
L'estimateur du maximum de vraisemblence v est alors obtenu en annulant la dérivée par rapport à v de cette expression il est égal à $\frac{1}{m}\sum_{1=1}^{m} u_i^2$

Ensuite 2 méthodes soit on fixe le taux de variance jour 2, soit variance targeting, on prend la variace de l'échantillon ou autre valeur pertinente pour imposer $V_L$($w=1-\alpha-\Beta$), il reste plus que 2 parametres ç estimer alpha et beta, pour retrouver EWMA :$w=0,\alpha=1-\lambda,\Beta=\lambda$


**Problèmes de convergence et choix des conditions initiales** 
 
Il n’existe pas de solution analytique à la maximisation de la vraisemblance dans le cas général des modèles GARCH. On doit donc recourir à une optimisation numérique de la vraisemblance ou de la pseudo vraisemblance suivant les cas. Dans la pratique, se pose alors le problème de la convergence de  l’algorithme  d’optimisation  retenu  et  plus  spécifiquement  du  problème  du  choix  de  conditions  initiales.  En  ce  qui  concerne  l’algorithme  d’optimisation,  deux  choix  sont  disponibles  dans  le  cadre  de  la  procédure  MODEL  sous  SAS  utilisée  pour  estimer  les  modèles  GARCH  :  la  méthode  de  Gauss  Newton Raphson et la méthode de Marquardt –Levenberg.  C’est cette dernière qui est utilisée dans nos programmes. Pour cet algorithme, nous avons retenu pour conditions initiales sur les paramètres $\alpha_i$ et $\beta_i$ les valeurs estimées dans le cas d’un modèle GARCH sous hypothèse de normalité. 

# ACF et PACF

ACF(autocorrélation de PEarson calculé entre une série et elle meme, décalé de k crans) is a measure of the correlation between the time series with a lagged version of itself. As the ACF values attenuate rapidly for increasing lags, we can assume the series is stationary. This is observed to estimate q.
(l'acf dans MA est censé etre 0)

PACF(autocorrélation partielle meme principe que l'autocorrélation totale mais en retirant la corrélation totale) measures the correlation between the time series with a lagged version of itself at k period, but after eliminating the variations already explained by the intervening comparisons, that is at lags less than k. This is observed to estimate p.(PACF AR est censé décroitre et tendre vers 0 avec le lag)

Number of lags is usually estimated from N/4. Where N = length of series.

**Selon Franses (1998), une série temporelle est non linéaire quand des chocs importants ont un impact différent de chocs de moindre envergure dans le sens que l’impact d’un choc n’est pas proportionnel à son importance. La non-linéarité peut aussi signifier que l’impact d’un choc dépend de son signe. Pour modéliser le caractère non linéaire des séries temporelles, nous envisageons ici deux méthodes d’estimation : la méthode STAR, soit un processus autorégressif de transition lisse ou sans cassures (smooth), et le modèle économétrique basé sur les réseaux de neurones, que nous désignerons par ANN.**

## TESTS D’AUTOCORRÉLATION

Hypothese sous-jacente est que la volatiltié change au cours du temps quand $R_i^2$ est elévé $R_{i+1}^2$ à tendance à etre élévé aussi réciproquement l'inverse quand il l'est faible, Si **GARCH fonctionne l'autocorrélation devrait disparaitre**, on peut tester en étudiant la structure d'autocorrélations des variables
$\frac{r_i^2}{\sigma_i^2}$, si elle a diminué

### test Q de Ljung-Box 

**La statistique de Ljung-Box** offre a possibilité d'effectuer un test plus spécifique pour une série composé de m observations la statistique est :
$$
m\sum_{k=1}^{k} w_kn_k^2 \\
w_k=\frac{m+2}{m-K}
$$
Avec $n_k$ l'autocorrélation pour le décalage k et K le nb de décalage considérés, pour K=15 rejeté au seuil de 5% si la statistique de LB est $>25$

$H_0 :e(1)=e(2=)=..=0$
#### Test de Durbin-Watson

Le test de Durbin-Watson est un test statistique destiné à tester l'autocorrélation des résidus dans un modèle de régression linéaire




##  LA MÉTHODE DE BOX ET JENKINS

La méthode de Box et Jenkins consiste à déceler la forme du modèle ARIMA qui reproduit le mieux la série financière analysée. Cette méthode comporte trois étapes : i) l’identification ; ii) l’estimation ;iii) les tests et diagnostics

### i) L’identification

L’identification se base principalement sur l’analyse des ACF et PACF des séries économiques et financières considérées. On peut distinguer les cas d’espèce suivants :



- 1) Si l’ACF décroît lentement vers 0, c’est-à-dire que pour des retards éloignés, les coefficients d’autocorrélation demeurent significatifs, on considère alors que cette série est non stationnaire. On devra alors différencier cette série une fois et parfois même deux fois pour la rendre stationnaire. Un modèle ARMA d’ordre p et q peu élevé pourra être ensuite estimé sur la série stationnarisée.
- 2) Pour un processus MA(q), l’ACF : p(k) est égale à 0 pour k > q et la PACF décroît géométriquement vers 0. Pour déterminer l’ordre q de ce processus, il faut comparer l’ACF : p(k) avec +/- (2/$\sqrt{T}$)
- 3) Pour un processus AR(p), la PACF :  $\theta_{kk}$ est égale à 0 pour k > p et l’ACF décroît géométriquement vers 0. Pour déterminer l’ordre p de ce processus, on compare la PACF :$\theta_kk$ avec +/- (2/$\sqrt{T}$)
- 4) Si l’on ne trouve pas de point de rupture précis, un modèle ARMA pourrait être pertinent. Par exemple, étant donné qu’un ARMA(1, 1) est une combinaison d’un modèle AR et d’un modèle MA, on s’attend à ce que l’ACF ait les caractéristiques des modèles AR et MA combinés. La portion MA disposant d’une mémoire d’une période seulement, le point de rupture devrait se produire après une période. Par ailleurs, la composante AR dispose d’une ACF qui décroît géométriquement et on anticipe ce profil pour des retards supérieurs
à une période.

### ii) L’estimation

Si un modèle AR a été identifié, alors l’estimation se fera par les
MCO. Par ailleurs, si on a identifié un modèle MA, l’estimation se fera par les moindres carrés non linéaires (NLS) ou par la méthode ML. Si un modèle ARMA a été identifié, l’estimation s’effectuera par
la méthode des NLS ou la méthode ML, ou encore par la méthode des MCO en deux étapes11.

### iii) Tests et diagnostics

Dans cette sous-section, nous traitons de la surparamétrisation (overfitting) et des tests sur les résidus.

Dans cette sous-section, nous traitons de la surparamétrisation (overfitting) et des tests sur les résidus. 

**La surparamétrisation:**

Si un modèle ARIMA(p, d, q) a été identifié, la surparamétrisation
consiste à estimer un modèle ARIMA(p + 1, d, q) ou un modèle ARIMA (p, d, q + 1) ou les deux et faire un test sur le ou les paramètres additionnels. Si le vrai modèle est un ARIMA(p, d, q), les tests sur les paramètres additionnels ne devraient pas être significativement différents de 0.

**Analyse des résidus:**

Une fois l’estimation des modèles ARIMA complétée, on calcule les résidus de façon à dégager les ACF et PACF de ces résidus. On devrait trouver que la fonction d’autocorrélation est non significative pour tous les retards en comparant $ρ_k$ avec $+/-$(2/$\sqrt{T}$).
Une procédure plus scientifique est de calculer le test Ljung-Box (1978).


###  Autres Critères de Sélection pour les modèles :AIC et SC

Le critère d’Akaike AIC(1973) et celui de Schwarz SC (1978).


### ÉVALUATION DE LA PRÉCISION DES PRÉVISIONS

le RMSE (root mean square error) ; 2) le MAE (mean absolute error) ;les deux statistiques U de Theil. Ces critères donnent lieu au choix d’un modèle qui minimise lesdits critères.

## Prévision des modèles ARCH

$$
y_t=x_t \Beta+\epsilon_t
$$
1. On estime d’abord l’équation précédente par les MCO.(on estime $\epsilon_t$ par MCO)
2. Les résidus estimés ($\epsilon^2$) de la régression précédente sont alors élevés au carré :
**On cacule acf et pacf sur $\epsilon_t^2$ 
Si les fonctions acf ou pacf présentent des autocorrélations non nulles pour des retards supérieurs à 0, alors le processus $\epsilon_t$ présente des caractéristiques d'un processus hétéroscédastiques, on proceède à l'estimation d'un processus GARCH(p,q)**
3. On effectue la régression artificielle suivante :$\epsilon^2=\alpha_0+\alpha_1\epsilon_{t-1}^2+..+\alpha_q \epsilon_{t-q}^2+u_t$
On se sert du R2 de cette régression pour effectuer le test. En effet, la
statistique $(T\times R^2)\mapsto \chi ^2(q)$
S’il n’y a pas d’effet ARCH, le $R^2$ de la
régression artificielle sera faible. La statistique $(T\times R^2)$ se situera
alors sous la valeur critique de la distribution $\chi ^2(q)$. L’hypothèse H0, à
l’effet de l’absence d’effet ARCH, se formule comme suit : $\alpha_1=\alpha_q=0$.
Par exemple, on rejettera l’hypothèse de l’absence de l’effet ARCH(1) si $(T\times R^2)>\chi^2(1)$.  


**Statisque Q** estimation ordre p et q

$$
Q(s)=T(T+2)\sum_{k=1}^s \frac{\overline{p}_k^2(\epsilon_t^2)}{T-k}
$$
Ou $\overline{p}_k^2(\epsilon_t^2)$ est le coeff d'autocorrélation estimé pour le retard k et T nb d'observations.
$Q(s)$ suit la règle du chi-2 avec s degré de liberté, si reste significative pour de larges valeurs de s alors on a un modèle GARCH, si que pour un faible retard s alors modèle ARCH.
- 4) une fois les paramètres p et q déterminés on estime de manière classique les paramètres en utilisant la méthode du quasi-maximum de vraisemblance.
En posant $\epsilon_t-x_t^T\beta$
$$
l=-\frac{T}{2}ln(2\pi)-\frac{1}{2}\sum_{t=1}^T -\frac{\epsilon_t^2}{\sigma_t^2}
$$



révision à travers une modélisation ARCH sera dite meilleure si elle minimise le MAPE  (Mean  Absolute  Pourcentage  Error)  et  présente  un  coefficient  de  Theil proche  de  zéro:  faut-il  encore  que  les  résidus  de  la  variance  conditionnelle  des erreurs soient des bruits blancs (gaussiens).

A titre de vérification on peut calculer à nouveau la statistique $Q(s)$ pour $\epsilon_t^*=\epsilon_t/\sigma_t$, si les résidus standardisés sont bien décorrélés.

## Prévision volatiltié future

Plus nous nous projetons dans le futur et plus les prévisions du taux de variances tendent vers $V_L$
$$
\sigma_n^2=(1-\alpha-\beta)V_L+\alpha U_{n-1}^2+\beta \sigma_{n-1}^2 \\
\sigma_{n+t}^2-V_L=+\alpha(U_{n+t-1}^2-V_L)+\beta(\sigma_{n+t-1}^2-V_L) \\
E(\sigma_{n+t}^2-V_L)=(\alpha+\beta)^t(\sigma_n^2-V_L) \\
E(\sigma_{n+t}^2)=V_L+(\alpha+\beta)^t(\sigma_n^2-V_L)
$$
### Structure par termes des volatilités

$$
V(t)=E(\sigma_{n+t}^2) \\
a=ln(\frac{1}{\alpha+\beta})\\
V(t)=V_L+e^{-at}[V(0)-V_L]\\
$$
V(t) est une estimation du taux de variance instantanée dans t jours, le taux de variance moyen entre aujourd'hui et la date T est alors:
$$
\frac{1}{T}\int_{0}^{T}V(t)dt=V_L+\frac{1-e^{-at}}{aT}[V(0)-V_L]
$$
Plus l'opt a écheance éloigné plus cette expression est proche de $V_L$.
$$
\sigma(T)^2=252(V_L+\frac{1-e^{-at}}{aT}[V(0)-V_L])
$$
Comme on l'a vu les primes des differents contrats d'options sur un meme actif SJ sont utilisés pour exxtraire une structure par terme des volatilités (relation VI et maturité)
On peut utiliser cette expression pour la structure par terme
Quand la vol courante est $>$ à $V_L$ (vol long terme), le modèle GARCH conduit à une estimation d'une structure par terme des volatilités décroissante et inversement


De Nombreuses instituions se fondent sur des analyses semblables pour déterminer l'exposition de leurs positions aux variations de volatilité, au lieu de considerer une hausse uniforme de 1% de la VI pour le calcul du vega

# Modèle IGARCH (1987)
Eggle & Bollerserv
**à mémoire longue**

C’est un modèle ARCH non linéaire caractérisé par  un  effet  de  persistance  dans  la  variance.  C’est-à-dire  qu’un  choc  sur  la  variance  conditionnelle  actuelle se répercute sur toutes les valeurs futures prévues

- correspondent au cas d’une racine unitaire dans le processus de variance conditionnelle
- sont caractérisés par un effet de persistance dans la varianceUn processusεtsatisfait une représentation IGARCH(p,q) si et seulement si 
$$
 \sigma_t^2= \omega+ \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2 +\sum_{i=1}^p \beta_i \sigma_{t-i}
$$

voir aussi FIGARCH pour mémoire longue

# Modèle EGARCH (1986)

Modèles  géneralisés par Nelson (1991)

**Pour prise en compte asymetrie**

C’est un modèle ARCH non linéaire qui permet de rendre compte de l’asymétrie dans  la  réponse  de  la  variance  conditionnelle  à  une  innovation.

Plus  précisément,  le  modèle  EGARCH autorise une forme d’asymétrie qui dépend non seulement du signe positif ou négatif de l’innovation, mais aussi de l’amplitude de ce choc.

Par ailleurs le modèle EGARCH présente en outre l’avantage,  par  rapport  au  modèle  GARCH  standard,  de  ne  nécessiter  aucune  restriction  de  non  négativité sur les paramètres afin de garantir la positivité de la variance conditionnelle. 


Le modèle EGARCH (exponential GARCH), dû à Nelson (1991), a été développé pour modéliser non seulement **l’excès de leptokurticité** mais aussi **les effets asymétrique**s qu’ont les rendements sur la volatilité. Dans la littérature, cet effet est appelé : ***leverage effect***. Il s’agit d’une **corrélation négative entre les rendements présents et la volatilité future**. En effet, une baisse du rendement d’une action est associée à une diminution de la valeur marchande de l’avoir des actionnaires de l’entreprise qui l’a émise. Il en résulte une hausse du levier de cette entreprise, soit le ratio de sa dette à l’équité. L’entreprise est alors perçue plus risquée sur le marché, ce qui augmente la volatilité future du rendement de cette action. La baisse du rendement de cette action exerce donc un effet de levier sur sa volatilité future.




Ce sont les chocs epsilon qui modifie la variance, dans arch classique, elle dépend de leurs amplitudes et non de leurs signes, or chocs négatifs plus d'impacts que chocs positifs
$$
\epsilon_t = z_t\sigma \\
 ln\sigma_t^2= \omega+ \beta ln\sigma_{t-i}^2+ \alpha \mid \frac{\epsilon_{t-i}}{\sigma_{t-i}}\mid + \gamma \mid \frac{\epsilon_{t-i}}{\sigma_{t-i}}\mid  \\
$$
ou
$$
 ln\sigma_t^2= \omega+ \beta ln\sigma_{t-i}^2+\alpha_ig(z_{t-i})\\
 g(z_{t-i})=\theta z_{t-i}+\gamma(|z_{t-i}|-E|z_{t-i}|)
$$

voir validité du modèle

Les  méthodes  généralement  retenues  sont  celles  du  maximum de vraisemblance (MV) ou du pseudo maximum de vraisemblance (PMV). L’avantage du PMV  réside  dans  le  fait  que  l’estimateur  obtenu  converge  malgré  une  mauvaise  spécification  (supposée  normale)  de  la  distribution  conditionnelle  des  résidus,  à  condition  que  la  loi  spécifiée  appartienne à la famille des lois exponentielles (Gouriéroux et Montfort, 1989). Ainsi, l’estimateur du MV obtenu sous l’hypothèse de normalité des résidus et l’estimateur du PMV sont identiques, seules leurs  lois  asymptotiques  respectives  diffèrent.    Toutefois  dans  les  deux  cas  (MV  ou  PMV),  sous  les  hypothèses standards, l’estimateur est asymptotiquement convergent et asymptotiquement normal.


# GJR-GARCH
**Pour prise en compte asymetrie**

C’est un modèle ARCH non linéaire qui permet de rendre compte de l’asymétrie dans  la  réponse  de  la  variance  conditionnelle  à  une  innovation.

 La logique de ce modèle est similaire à celle des modèles à changements  de  régimes  et  plus  spécifiquement  des  modèles  à  seuils  (Tong,  1990).  Le  principe  du  modèle  GJR-GARCH  est  que  la  dynamique  de  la  variance  conditionnelle  admet  un  changement  de  régime qui dépend du signe de l’innovation passée. 
 
 
$$r_t= \mu + \phi r_{t-1} + \sigma_t z_t$$
$$\sigma_t^2 =w+\sum_{i=1}^{p}(\alpha_i \epsilon_{t-i}^2+\gamma I_{(\epsilon_{t-i<0})}\epsilon_{t-i}^2)+\sum_{j=1}^{q}\beta_j\sigma_{t-j}^2$$

$$\sigma_t^2 = \omega + \alpha\epsilon_{t-1}^2 + \beta\sigma_{t-1}^2 + \gamma\epsilon_{t-1}^2 I_{(\epsilon_{t-1} < 0)},$$
$$z_t \sim N,SN,T,ST,GED,SGED.$$

$I_{(\epsilon_{t-i<0})}=1$ si $\epsilon_{t-i}<0$
$I_{(\epsilon_{t-i<0})}=0$ sinon

The conditional mean equation (first equation) is governed by the autoregressive process with 1st-degree lag, and the conditional variance equation is the GJR-GARCH model of Glosten, Jagannathan and Runkle (1993). It is an extension of Bollerslev’s (1986) famous GARCH (Generalised Autoregressive Conditional Heteroskedasticity) model, with the additional advantage of considering cases in which the influence of positive and negative innovations on the variance is different. In other words, parameter alpha measures the effect of positive innovations, while alpha + gamma is used in case of negative innovations. GARCH model is obtained by setting gamma of GJR-GARCH to 0.

# APARCH MODEL

# Modèles Garch à seuil

## Modèle TARCH (Threshold)

Le modèle TARCH (threshold ARCH), dû à Zakoian (1994)

Bonnes nouvelles($\epsilon >0$, d=O) et les mauvaises n'ont pas la meme impact sur la variance conditionnelle.
Ce modèle veut intégrer l’observation suivante sur les séries temporelles des rendements financiers. En effet, on a remarqué que les **mauvaises nouvelles affectent davantage la volatilité que les bonnes**.

$$\sigma_t	= \omega + \alpha|\epsilon_{t-1}| + \beta|\sigma_{t-1}| + \gamma|\epsilon_{t-1}| I_{(\epsilon_{t-1} < 0)},$$


$$
 \sigma_t^2=  \omega+\alpha \epsilon_{t-i}^2+\gamma \epsilon_{t-i}^2+ d_{t-1}  \\
 d_t=1 \text{ Si } \epsilon_t<0 \text{ Sinon } d=0
$$

## QGARCH-Quadratic Garch

## Modèle LST-GARCH (logistic smooth transition)

## Modèle VS-Garch (Volatility Switching Garch)


L'ampleur est le plus important
petits chocs, positifs plus d'impact que négatif
grands chocs, l'inverse


## Modèle M-Garch

**LE plus utilisé en finance car le plus pertinent**
faire dépendre l'esperence du rendement à la  la volatilité, car plus vol augmente, plus inversion au risque augmente et rendement augmente


Le modèle GARCH-M (General Autoregrssive Conditional Heteroskedasticity in Mean) a été introduit par Engle-Lilien-Robbins  (1987). C’est  un  modèle  ARCH  qui  permet  de  mesurer  l’influence  du  rendement des titres sur la volatilité conditionnelle. Comme les modèles ARCH linéaires, le modèle GARCH-M repose sur une spécification quadratique de la variance conditionnelle des perturbations.
![e19af7300a3f8ad693f6d9bdb2a5f316.png](:/9690df01bab54c55aec1136e17c9a3ad)

Le  paramètre δmesure  l’impact  de  la  variance  conditionnelle  sur  le  rendement  ;  il  correspond  également  au  coefficient  de  l’aversion  relative  au  risque.  Les  variables  tz  sont  indépendamment  et  identiquement distribuées (i.i.d.). 